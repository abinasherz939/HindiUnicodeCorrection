{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9886c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sentencepiece as spm\n",
    "logger = logging.getLogger('sentencepiece')\n",
    "logger.setLevel(logging.CRITICAL) \n",
    "logger.propagate = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f53f32",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe642039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "हिंदी में 123 यह एक उदाहरण है इसे साफ किया जाना चाहिए।\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_hindi_text(text):\n",
    "\n",
    "    # Removed non-essential punctuation\n",
    "    text = re.sub(r\"[^\\w\\s\\u0900-\\u097F]\", \"\", text) \n",
    "    text = re.sub(r\"[a-zA-Z]\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "hindi_text = \"हिंदी में 123 यह एक उदाहरण है! इसे साफ, किया जाना चाहिए।\"\n",
    "cleaned_text = clean_hindi_text(hindi_text)\n",
    "print(cleaned_text) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb8b84",
   "metadata": {},
   "source": [
    "## Question 1.\n",
    "Perform the Unicode correction as discussed in class. Essentially, consonants with a\n",
    "halant character should be counted as 1, while those without that should be counted as 2. You\n",
    "may transliterate the corpus to ISO15919 format or ITRANS before and/or after performing the\n",
    "correction.\n",
    "Output the code to transform a sentence as a list of characters. This will be tested against random\n",
    "inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abbcf1",
   "metadata": {},
   "source": [
    "## Reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3294643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'corpus.txt'  \n",
    "try:\n",
    "    with open(file_path, 'r',encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "        lines = data\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "with open(file_path,'r',encoding='utf-8') as f:\n",
    "    line1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4ac109",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_hindi_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc54594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "consonants = {\n",
    "    \"क\": \"k\", \"ख\": \"kh\", \"ग\": \"g\", \"घ\": \"gh\", \"ङ\": \"N\",\n",
    "    \"च\": \"ch\", \"छ\": \"chh\", \"ज\": \"j\", \"झ\": \"jh\", \"ञ\": \"~N\",\n",
    "    \"ट\": \"T\", \"ठ\": \"Th\", \"ड\": \"D\", \"ढ\": \"Dh\", \"ण\": \"N\",\n",
    "    \"त\": \"t\", \"थ\": \"th\", \"द\": \"d\", \"ध\": \"dh\", \"न\": \"n\",\n",
    "    \"प\": \"p\", \"फ\": \"ph\", \"ब\": \"b\", \"भ\": \"bh\", \"म\": \"m\",\n",
    "    \"य\": \"y\", \"र\": \"r\", \"ल\": \"l\", \"व\": \"v\", \"श\": \"sh\",\n",
    "    \"ष\": \"Sh\", \"स\": \"s\", \"ह\": \"h\", \"ळ\": \"L\", \"क्ष\": \"kSh\",\n",
    "    \"ज्ञ\": \"j~n\", \"ड़\": \"R\", \"य़\": \"y\", \"ज़\": \"z\", \"ब़\": \"b\", \"क़\": \"q\", \"ख़\": \"Kh\", \"ग़\": \"G\",\n",
    "    \"ड़\": \"R\", \"ढ़\": \"Rh\", \"फ़\": \"f\", \"श़\": \"sh\", \"ऋ\": \"Ri\", \"ॠ\": \"Ri\", \"ॡ\": \"Li\", \"ऌ\": \"Li\", \"ऴ\": \"L\", \"ॐ\": \"OM\",\n",
    "    \"ऽ\": \"'\"\n",
    "         \"\"\n",
    "}\n",
    "hindi_to_english_transliteration = {\n",
    "    # Vowels\n",
    "    \"अ\": \"a\", \"आ\": \"aa\", \"इ\": \"i\", \"ई\": \"ii\", \"उ\": \"u\",\n",
    "    \"ऊ\": \"uu\", \"ऋ\": \"RRi\", \"ॠ\": \"RRi\", \"ऌ\": \"LLi\", \"ॡ\": \"LLi\",\n",
    "    \"ए\": \"e\", \"ऐ\": \"ai\", \"ओ\": \"o\", \"औ\": \"au\", \"अं\": \"am\", \"अः\": \"ah\",\n",
    "}\n",
    "matras = {\"ा\": \"आ\", \"ि\": \"इ\", \"ी\": \"ई\", \"ु\": \"उ\", \"ू\": \"ऊ\", \"ृ\": \"ऋ\", \"ॄ\": \"ॠ\", \"ॢ\": \"ऌ\", \"ॣ\": \"ॡ\", \"े\": \"ए\", \"ै\": \"ऐ\",\n",
    "          \"ो\": \"ओ\", \"ौ\": \"औ\", \"ं\": \"अं\", \"ः\": \"अः\", \"ँ\": \"अँ\"}\n",
    "reverse_matras = {\"अ\": \"\", \"आ\": \"ा\", \"इ\": \"ि\", \"ई\": \"ी\", \"उ\": \"ु\", \"ऊ\": \"ू\", \"ऋ\": \"ृ\", \"ॠ\": \"ॄ\", \"ऌ\": \"ॢ\", \"ॡ\": \"ॣ\",\n",
    "                  \"ए\": \"े\", \"ऐ\": \"ै\", \"ओ\": \"ो\", \"औ\": \"ौ\", \"अं\": \"ं\", \"अः\": \"ः\", \"अँ\": \"ँ\"}\n",
    "halanta = \"्\"\n",
    "halanta_char = []\n",
    "second_matras = {\"अं\": \"ं\", \"अः\": \"ः\", \"अँ\": \"ँ\"}\n",
    "reverse_matras_second = {\"ं\": \"अं\", \"ः\": \"अः\", \"ँ\": \"अँ\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74aa10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_conversion(line):\n",
    "    output_characters = []\n",
    "    for lineIndex in range(len(line)):\n",
    "        if line[lineIndex] in consonants and lineIndex + 1 < len(line) and line[lineIndex + 1] != '्':\n",
    "            if line[lineIndex + 1] in matras:\n",
    "                output_characters.append(line[lineIndex])\n",
    "                output_characters.append(matras[line[lineIndex + 1]])\n",
    "                lineIndex += 1\n",
    "                if lineIndex+1<len(line) and line[lineIndex + 1] in matras:\n",
    "                    output_characters.append(matras[line[lineIndex + 1]])\n",
    "                    lineIndex += 1\n",
    "            elif line[lineIndex + 1] == \"़\":\n",
    "                output_characters.append(line[lineIndex] + \"़\")\n",
    "                if lineIndex + 2 < len(line) and line[lineIndex + 2] in second_matras:\n",
    "                    output_characters.append(matras[line[lineIndex + 2]])\n",
    "                    lineIndex += 2\n",
    "                else:\n",
    "                    output_characters.append('अ')\n",
    "            else:\n",
    "                output_characters.append(line[lineIndex])\n",
    "                output_characters.append('अ')\n",
    "        elif line[lineIndex] in consonants and lineIndex + 1 < len(line) and line[lineIndex + 1] == '्':\n",
    "            val = line[lineIndex]\n",
    "            val += \"्\"\n",
    "            output_characters.append(val)\n",
    "            lineIndex += 1\n",
    "        elif line[lineIndex] in hindi_to_english_transliteration:\n",
    "            output_characters.append(line[lineIndex])\n",
    "        else:\n",
    "            continue \n",
    "    return output_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63606848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_convert(syllable_fragment):\n",
    "    current_segment = \"\"\n",
    "    output_segments = []\n",
    "    i = 0\n",
    "    while i < len(syllable_fragment):\n",
    "        if syllable_fragment[i] in consonants:\n",
    "            current_segment = current_segment + syllable_fragment[i]\n",
    "        elif syllable_fragment[i] in halanta_char:\n",
    "            current_segment = current_segment + syllable_fragment[i]\n",
    "        else:\n",
    "            if current_segment != \"\":\n",
    "                if syllable_fragment[i] in reverse_matras:\n",
    "                    current_segment = current_segment + reverse_matras[syllable_fragment[i]]\n",
    "                if i + 1 < len(syllable_fragment) and syllable_fragment[i + 1] in second_matras:\n",
    "                    current_segment = current_segment + second_matras[syllable_fragment[i + 1]]\n",
    "                    i = i + 1\n",
    "            else:\n",
    "                current_segment = syllable_fragment[i]\n",
    "            output_segments.append(current_segment)\n",
    "            current_segment = \"\"\n",
    "        i += 1\n",
    "    return [segment for segment in output_segments if segment] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1286a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def addHal(output_characters):\n",
    "#     for i in range(len(output_characters)):  \n",
    "#         char = output_characters[i]\n",
    "#     if char in consonants:\n",
    "#         output_characters[i] += halanta \n",
    "#     return output_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5a71f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['बा', 'रि', 'श', 'के', 'मौ', 'स', 'म', 'में', 'लो', 'ग', 'ग', 'र', 'म', 'चा', 'य', 'का', 'आ', 'नं', 'द', 'ले', 'ते', 'हैं']\n",
      "['ब', 'आ', 'र', 'इ', 'श', 'अ', 'क', 'ए', 'म', 'औ', 'स', 'अ', 'म', 'अ', 'म', 'ए', 'अं', 'ल', 'ओ', 'ग', 'अ', 'ग', 'अ', 'र', 'अ', 'म', 'अ', 'च', 'आ', 'य', 'अ', 'क', 'आ', 'आ', 'न', 'अं', 'द', 'अ', 'ल', 'ए', 'त', 'ए', 'ह', 'ऐ', 'अं']\n"
     ]
    }
   ],
   "source": [
    "val = character_conversion('बारिश के मौसम में, लोग गरम चाय का आनंद लेते हैं।')\n",
    "print(syllable_convert(val))\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465a3f0",
   "metadata": {},
   "source": [
    "## Question 2.\n",
    "Find all characters and syllables. Store a list of them in descending order of their\n",
    "frequencies.\n",
    "Find the top-20 frequent uni-gram and bi-gram frequencies of characters and syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39f656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_top_k_frequencies(frequencies, k):\n",
    "    \"Prints the top 'k' items from a frequency dictionary.\" \n",
    "    sorted_freqs = sorted(frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "    for token, frequency in sorted_freqs[:k]:\n",
    "        print(f\"{token}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38795099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllable Unigram Frequency:\n",
      "र: 1173238\n",
      "क: 616759\n",
      "स: 518703\n",
      "न: 515812\n",
      "प: 405371\n",
      "के: 399507\n",
      "त: 349748\n",
      "ल: 333294\n",
      "ने: 325953\n",
      "म: 324879\n",
      "का: 300879\n",
      "ए: 288027\n",
      "या: 284755\n",
      "ह: 279939\n",
      "य: 277095\n",
      "में: 260017\n",
      "अ: 253390\n",
      "ब: 250789\n",
      "की: 236633\n",
      "ग: 231766\n",
      "Syllable Bigram Frequency:\n",
      "क र: 164160\n",
      "औ र: 115615\n",
      "प र: 100331\n",
      "प् र: 87696\n",
      "इ स: 83122\n",
      "ए क: 59498\n",
      "लि ए: 54068\n",
      "के लि: 50111\n",
      "न हीं: 47406\n",
      "अ प: 45206\n",
      "र ने: 44705\n",
      "त क: 44025\n",
      "का र: 42594\n",
      "र का: 41672\n",
      "कि या: 37119\n",
      "स के: 36660\n",
      "ता है: 36647\n",
      "ने के: 35325\n",
      "त र: 35025\n",
      "न के: 33096\n",
      "Character Unigram Frequency:\n",
      "अ: 7153466\n",
      "आ: 2966631\n",
      "ए: 2303828\n",
      "क: 2059393\n",
      "र: 1885117\n",
      "ई: 1445429\n",
      "इ: 1425788\n",
      "न: 1250679\n",
      "ह: 1130493\n",
      "स: 1115612\n",
      "अं: 1111292\n",
      "म: 1022292\n",
      "ओ: 891901\n",
      "ल: 862536\n",
      "त: 852911\n",
      "य: 752206\n",
      "प: 664385\n",
      "व: 604495\n",
      "उ: 586141\n",
      "द: 546709\n",
      "Character Bigram Frequency:\n",
      "र अ: 1173240\n",
      "अ क: 921468\n",
      "अ र: 786891\n",
      "क अ: 616852\n",
      "अ ह: 562925\n",
      "अ न: 551313\n",
      "स अ: 518739\n",
      "न अ: 515816\n",
      "अ म: 467030\n",
      "क ए: 407129\n",
      "प अ: 405636\n",
      "त अ: 353263\n",
      "अ स: 348242\n",
      "आ र: 343947\n",
      "ए अं: 341741\n",
      "ल अ: 333296\n",
      "अ त: 332006\n",
      "न ए: 328953\n",
      "म अ: 324890\n",
      "क आ: 314321\n"
     ]
    }
   ],
   "source": [
    "def process_line(line):\n",
    "    \"\"\"Processes a single line for syllable and character frequencies.\"\"\"\n",
    "    characters = character_conversion(line)\n",
    "    syllables = syllable_convert(characters)\n",
    "    return characters, syllables \n",
    "\n",
    "def update_frequencies(tokens, unigram_dict, bigram_dict):\n",
    "    \"\"\"Updates frequency dictionaries based on a list of tokens.\"\"\"\n",
    "    for token in tokens:\n",
    "        unigram_dict[token] = unigram_dict.get(token, 0) + 1  # Update unigram count \n",
    "\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = \" \".join((tokens[i], tokens[i + 1])) \n",
    "        bigram_dict[bigram] = bigram_dict.get(bigram, 0) + 1  # Update bigram count\n",
    "\n",
    "unigram_freq = {}\n",
    "unigram_freq_char = {}\n",
    "bigram_freq = {}\n",
    "bigram_freq_char = {}\n",
    "\n",
    "for line in line1: \n",
    "    characters, syllables = process_line(line)\n",
    "\n",
    "    update_frequencies(characters, unigram_freq_char, bigram_freq_char)\n",
    "    update_frequencies(syllables, unigram_freq, bigram_freq)\n",
    "\n",
    "print(\"Syllable Unigram Frequency:\")\n",
    "print_top_k_frequencies(unigram_freq, 20)\n",
    "\n",
    "print(\"Syllable Bigram Frequency:\")\n",
    "print_top_k_frequencies(bigram_freq, 20)\n",
    "\n",
    "print(\"Character Unigram Frequency:\")\n",
    "print_top_k_frequencies(unigram_freq_char, 20)\n",
    "\n",
    "print(\"Character Bigram Frequency:\")\n",
    "print_top_k_frequencies(bigram_freq_char, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9f556",
   "metadata": {},
   "source": [
    "# Problem-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390084c",
   "metadata": {},
   "source": [
    "Run the Unigram, BPE (vocabulary sizes, V = 1k, 2k), mBERT (max length = 1k,\n",
    "2k), IndicBERT (max length = 1k, 2k), and White-space tokenizers on the entire corpus. You\n",
    "may use Sentence Piece or similar libraries for this purpose.\n",
    "Find the unigram frequencies of tokens and bi-gram frequencies of tokens, syllables, and characters\n",
    "for each of the tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12af61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce480c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_file_and_print_top_k(model_name, vocab, k, frequencies):\n",
    "    token_types = ['unigram', 'bigram', 'syllable', 'char']  #To define order\n",
    "\n",
    "    for i, token_freqs in enumerate(frequencies):\n",
    "        token_type = token_types[i]\n",
    "        file_name = f\"{model_name}_{token_type}_{vocab}.txt\"\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            sorted_counts = sorted(token_freqs.items(), key=lambda item: item[1], reverse=True)\n",
    "            for token, count in sorted_counts:\n",
    "                f.write(f\"{token}: {count}\\n\")\n",
    "\n",
    "        print(f\"\\nTop {k} for {token_type.capitalize()}\\n\")\n",
    "        for token, count in sorted_counts[:k]:\n",
    "            print(f\"{token}: {count}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def count_frequencies(tokenizer, text_data):\n",
    "    encoded_tokens = tokenizer.encode(text_data, out_type=str)\n",
    "\n",
    "    # Unigrams and Bigrams\n",
    "    unigram_frequencies = Counter(encoded_tokens)\n",
    "    bigram_frequencies = Counter(zip(encoded_tokens, encoded_tokens[1:]))\n",
    "\n",
    "    # Syllables and Characters\n",
    "    syllables = []\n",
    "    characters = []\n",
    "    for token in encoded_tokens:\n",
    "        syllables.extend(syllable_convert(token)) \n",
    "        characters.extend(character_conversion(token))  \n",
    "\n",
    "    bigram_syllables = zip(syllables, syllables[1:])\n",
    "    bigram_characters = zip(characters, characters[1:])\n",
    "\n",
    "    syllable_frequencies = Counter(bigram_syllables)\n",
    "    character_frequencies = Counter(bigram_characters)\n",
    "\n",
    "    return unigram_frequencies, bigram_frequencies, syllable_frequencies, character_frequencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320031d8",
   "metadata": {},
   "source": [
    "### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2419e00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 23:23:17.653368: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def train_and_load_tokenizer(file_path, model_prefix, model_type, vocab_size):\n",
    "    \n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=file_path,\n",
    "        model_prefix=model_prefix,\n",
    "        model_type=model_type,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "\n",
    "    model_file = f\"{model_prefix}.model\"\n",
    "    return spm.SentencePieceProcessor(model_file)\n",
    "\n",
    "def train_calculate_and_output(file_path, model_prefix, model_type, vocab_size, output_model_name):\n",
    "    tokenizer = train_and_load_tokenizer(file_path, model_prefix, model_type, vocab_size)\n",
    "    frequencies = count_frequencies(tokenizer, data)\n",
    "    freq_list = list(frequencies)  \n",
    "    write_to_file_and_print_top_k(output_model_name, vocab_size, 10, freq_list)\n",
    "    return tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8685d6",
   "metadata": {},
   "source": [
    "## Unigram for 1000 vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b919dc79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 for Unigram\n",
      "\n",
      "▁के: 162\n",
      "न: 155\n",
      "।: 138\n",
      "र: 138\n",
      "▁है: 135\n",
      "▁में: 135\n",
      "ा: 129\n",
      "ल: 126\n",
      "ी: 108\n",
      "▁की: 103\n",
      "\n",
      "Top 10 for Bigram\n",
      "\n",
      "('▁है', '।'): 52\n",
      "('▁के', '▁लिए'): 26\n",
      "('▁है', '▁कि'): 17\n",
      "('▁ज', 'न'): 17\n",
      "('▁हैं', '।'): 14\n",
      "('ता', '▁है'): 12\n",
      "('न', 'ि'): 12\n",
      "('▁गया', '▁है'): 11\n",
      "('▁कहा', '▁कि'): 11\n",
      "('न', 'तंत्र'): 11\n",
      "\n",
      "Top 10 for Syllable\n",
      "\n",
      "('▁', 'क'): 577\n",
      "('क', '▁'): 542\n",
      "('ं', '▁'): 413\n",
      "('▁', 'ह'): 297\n",
      "('▁', 'म'): 226\n",
      "('▁', '▁'): 207\n",
      "('न', '▁'): 200\n",
      "('▁', 'स'): 190\n",
      "('त', '▁'): 165\n",
      "('स', '▁'): 157\n",
      "\n",
      "Top 10 for Char\n",
      "\n",
      "('क', 'ए'): 204\n",
      "('आ', 'क'): 188\n",
      "('ह', 'ऐ'): 178\n",
      "('न', 'ए'): 169\n",
      "('क', 'अ'): 158\n",
      "('अ', 'ह'): 153\n",
      "('य', 'आ'): 150\n",
      "('अ', 'क'): 148\n",
      "('ए', 'अं'): 142\n",
      "('म', 'ए'): 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: corpus.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5994 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 298383 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 456 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=39649689\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=153\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 298381 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=18991333\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 395429 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 298381\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 324920\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 324920 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=156332 obj=10.8621 num_tokens=702942 num_tokens/piece=4.49647\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=134610 obj=8.92531 num_tokens=703641 num_tokens/piece=5.22726\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=100903 obj=8.88768 num_tokens=727710 num_tokens/piece=7.21198\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=100760 obj=8.87633 num_tokens=727853 num_tokens/piece=7.22363\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75563 obj=8.91628 num_tokens=768262 num_tokens/piece=10.1672\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=75557 obj=8.90731 num_tokens=768256 num_tokens/piece=10.1679\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=56667 obj=8.96362 num_tokens=813830 num_tokens/piece=14.3616\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=56667 obj=8.95147 num_tokens=813800 num_tokens/piece=14.3611\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=42500 obj=9.0328 num_tokens=863216 num_tokens/piece=20.311\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=42500 obj=9.0174 num_tokens=863193 num_tokens/piece=20.3104\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=31875 obj=9.12149 num_tokens=915167 num_tokens/piece=28.7111\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=31875 obj=9.10142 num_tokens=915173 num_tokens/piece=28.7113\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=23906 obj=9.23421 num_tokens=969065 num_tokens/piece=40.5365\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=23906 obj=9.21287 num_tokens=969074 num_tokens/piece=40.5369\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=17929 obj=9.37773 num_tokens=1024155 num_tokens/piece=57.1228\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=17929 obj=9.34571 num_tokens=1024169 num_tokens/piece=57.1236\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=13446 obj=9.55358 num_tokens=1079886 num_tokens/piece=80.3128\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=13446 obj=9.51426 num_tokens=1079931 num_tokens/piece=80.3162\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter"
     ]
    }
   ],
   "source": [
    "# Unigram with Vocabulary 1000\n",
    "unigram_1000 = train_calculate_and_output(file_path, 'unigram_model', 'unigram', 1000, 'unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c535b",
   "metadata": {},
   "source": [
    "## Unigram for 2000 vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85a34f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=0 size=10084 obj=9.75948 num_tokens=1136993 num_tokens/piece=112.752\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10084 obj=9.7122 num_tokens=1137021 num_tokens/piece=112.755\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=7563 obj=9.9979 num_tokens=1195057 num_tokens/piece=158.014\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7563 obj=9.94147 num_tokens=1195156 num_tokens/piece=158.027\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5672 obj=10.269 num_tokens=1256218 num_tokens/piece=221.477\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5672 obj=10.2032 num_tokens=1256276 num_tokens/piece=221.487\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4254 obj=10.57 num_tokens=1322515 num_tokens/piece=310.887\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4254 obj=10.4928 num_tokens=1322548 num_tokens/piece=310.895\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3190 obj=10.9027 num_tokens=1393553 num_tokens/piece=436.85\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3190 obj=10.8123 num_tokens=1393577 num_tokens/piece=436.858\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2392 obj=11.271 num_tokens=1471177 num_tokens/piece=615.041\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2392 obj=11.1744 num_tokens=1471222 num_tokens/piece=615.059\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1794 obj=11.6747 num_tokens=1543976 num_tokens/piece=860.633\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1794 obj=11.5641 num_tokens=1543988 num_tokens/piece=860.64\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1345 obj=12.1125 num_tokens=1623113 num_tokens/piece=1206.78\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1345 obj=11.9912 num_tokens=1623111 num_tokens/piece=1206.77\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1100 obj=12.3859 num_tokens=1678184 num_tokens/piece=1525.62\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1100 obj=12.2959 num_tokens=1678185 num_tokens/piece=1525.62\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: unigram_model.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: unigram_model.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  ha"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 for Unigram\n",
      "\n",
      "▁के: 155\n",
      "।: 137\n",
      "▁है: 135\n",
      "▁में: 135\n",
      "▁की: 99\n",
      "ी: 82\n",
      "ल: 76\n",
      "▁को: 74\n",
      "न: 67\n",
      "▁का: 65\n",
      "\n",
      "Top 10 for Bigram\n",
      "\n",
      "('▁है', '।'): 52\n",
      "('▁के', '▁लिए'): 26\n",
      "('▁है', '▁कि'): 17\n",
      "('▁हैं', '।'): 14\n",
      "('▁गया', '▁है'): 11\n",
      "('▁कहा', '▁कि'): 11\n",
      "('▁जन', 'तंत्र'): 11\n",
      "('▁के', '▁साथ'): 10\n",
      "('।', '▁इस'): 9\n",
      "('ो', 'ज'): 9\n",
      "\n",
      "Top 10 for Syllable\n",
      "\n",
      "('▁', 'क'): 574\n",
      "('क', '▁'): 530\n",
      "('ं', '▁'): 411\n",
      "('▁', 'ह'): 306\n",
      "('▁', 'म'): 234\n",
      "('▁', '▁'): 207\n",
      "('▁', 'स'): 195\n",
      "('न', '▁'): 184\n",
      "('स', '▁'): 160\n",
      "('त', '▁'): 157\n",
      "\n",
      "Top 10 for Char\n",
      "\n",
      "('क', 'ए'): 204\n",
      "('आ', 'क'): 182\n",
      "('ह', 'ऐ'): 178\n",
      "('अ', 'क'): 176\n",
      "('क', 'अ'): 172\n",
      "('न', 'ए'): 169\n",
      "('अ', 'ह'): 168\n",
      "('र', 'अ'): 163\n",
      "('प', 'अ'): 153\n",
      "('ए', 'अं'): 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: corpus.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5994 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 298383 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 456 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=39649689\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=153\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 298381 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=18991333\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 395429 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 298381\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 324920\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 324920 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=156332 obj=10.8621 num_tokens=702942 num_tokens/piece=4.49647\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=134610 obj=8.92531 num_tokens=703641 num_tokens/piece=5.22726\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=100903 obj=8.88768 num_tokens=727710 num_tokens/piece=7.21198\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=100760 obj=8.87633 num_tokens=727853 num_tokens/piece=7.22363\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75563 obj=8.91628 num_tokens=768262 num_tokens/piece=10.1672\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=75557 obj=8.90731 num_tokens=768256 num_tokens/piece=10.1679\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=56667 obj=8.96362 num_tokens=813830 num_tokens/piece=14.3616\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=56667 obj=8.95147 num_tokens=813800 num_tokens/piece=14.3611\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=42500 obj=9.0328 num_tokens=863216 num_tokens/piece=20.311\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=42500 obj=9.0174 num_tokens=863193 num_tokens/piece=20.3104\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=31875 obj=9.12149 num_tokens=915167 num_tokens/piece=28.7111\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=31875 obj=9.10142 num_tokens=915173 num_tokens/piece=28.7113\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=23906 obj=9.23421 num_tokens=969065 num_tokens/piece=40.5365\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=23906 obj=9.21287 num_tokens=969074 num_tokens/piece=40.5369\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=17929 obj=9.37773 num_tokens=1024155 num_tokens/piece=57.1228\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=17929 obj=9.34571 num_tokens=1024169 num_tokens/piece=57.1236\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=13446 obj=9.55358 num_tokens=1079886 num_tokens/piece=80.3128\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=13446 obj=9.51426 num_tokens=1079931 num_tokens/piece=80.3162\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=10084 obj=9.75948 num_tokens=1136993 num_tokens/piece=112.752\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10084 obj=9.7122 num_tokens=1137021 num_tokens/piece=112.755\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=7563 obj=9.9979 num_tokens=1195057 num_tokens/piece=158.014\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7563 obj=9.94147 num_tokens=1195156 num_tokens/piece=158.027\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5672 obj=10.269 num_tokens=1256218 num_tokens/piece=221.477\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5672 obj=10.2032 num_tokens=1256276 num_tokens/piece=221.487\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4254 obj=10.57 num_tokens=1322515 num_tokens/piece=310.887\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4254 obj=10.4928 num_tokens=1322548 num_tokens/piece=310.895\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3190 obj=10.9027 num_tokens=1393553 num_tokens/piece=436.85\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3190 obj=10.8123 num_tokens=1393577 num_tokens/piece=436.858\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2392 obj=11.271 num_tokens=1471177 num_tokens/piece=615.041\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2392 obj=11.1744 num_tokens=1471222 num_tokens/piece=615.059\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=11.3072 num_tokens=1492220 num_tokens/piece=678.282\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=11.2792 num_tokens=1492210 num_tokens/piece=678.277\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: unigram_model.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: unigram_model.vocab\n"
     ]
    }
   ],
   "source": [
    "unigram_2000 = train_calculate_and_output(file_path, 'unigram_model', 'unigram', 2000, 'unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b4146",
   "metadata": {},
   "source": [
    "## BPE for 1000 vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5251e948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe_model\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: corpus.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5994 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 298383 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 456 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=39649689\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=153\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 298381 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 298381\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 324920\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1364729 min_freq=1990\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220725 size=20 all=7449 active=2256 piece=▁ल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117261 size=40 all=8844 active=3651 piece=िक\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74160 size=60 all=10391 active=5198 piece=▁थ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58450 size=80 all=12553 active=7360 piece=त्र\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43528 s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 for Unigram\n",
      "\n",
      "▁के: 162\n",
      "।: 137\n",
      "▁में: 135\n",
      "▁है: 134\n",
      "▁की: 103\n",
      "क: 75\n",
      "▁को: 75\n",
      "त: 66\n",
      "▁आ: 65\n",
      "न: 65\n",
      "\n",
      "Top 10 for Bigram\n",
      "\n",
      "('▁है', '।'): 52\n",
      "('▁के', '▁लिए'): 26\n",
      "('▁है', '▁कि'): 17\n",
      "('▁हैं', '।'): 14\n",
      "('▁गया', '▁है'): 11\n",
      "('▁कहा', '▁कि'): 11\n",
      "('▁जन', 'त'): 11\n",
      "('त', 'ंत्र'): 11\n",
      "('।', '▁इस'): 10\n",
      "('▁के', '▁साथ'): 10\n",
      "\n",
      "Top 10 for Syllable\n",
      "\n",
      "('▁', 'क'): 570\n",
      "('क', '▁'): 548\n",
      "('ं', '▁'): 412\n",
      "('▁', 'ह'): 304\n",
      "('▁', 'म'): 216\n",
      "('न', '▁'): 208\n",
      "('▁', '▁'): 207\n",
      "('▁', 'स'): 194\n",
      "('त', '▁'): 174\n",
      "('स', '▁'): 167\n",
      "\n",
      "Top 10 for Char\n",
      "\n",
      "('क', 'ए'): 198\n",
      "('आ', 'क'): 190\n",
      "('ह', 'ऐ'): 178\n",
      "('न', 'ए'): 172\n",
      "('क', 'अ'): 159\n",
      "('अ', 'क'): 152\n",
      "('य', 'आ'): 148\n",
      "('ए', 'अं'): 144\n",
      "('म', 'ए'): 141\n",
      "('आ', 'ह'): 135\n"
     ]
    }
   ],
   "source": [
    "# BPE with Vocabulary 1000\n",
    "bpe_1000 = train_calculate_and_output(file_path, 'bpe_model', 'bpe', 1000, 'bpe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f2af7",
   "metadata": {},
   "source": [
    "## BPE for 2000 vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3a27a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ize=100 all=14682 active=9489 piece=▁सं\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=43241 min_freq=4149\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35408 size=120 all=16900 active=3171 piece=▁किया\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28955 size=140 all=18442 active=4713 piece=न्ह\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24411 size=160 all=20405 active=6676 piece=▁राज\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21882 size=180 all=22297 active=8568 piece=▁सर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19406 size=200 all=24144 active=10415 piece=▁चु\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=19305 min_freq=2919\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17837 size=220 all=26375 active=3387 piece=गर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16215 size=240 all=27984 active=4996 piece=▁जी\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14802 size=260 all=29833 active=6845 piece=रो\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13436 size=280 all=31773 active=8785 piece=▁उप\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12360 size=300 all=32981 active=9993 piece=▁पुलिस\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12356 min_freq=1941\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11890 size=320 all=34340 active=2990 piece=▁इसके\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10951 size=340 all=35982 active=4632 piece=चार\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10320 size=360 all=37937 active=6587 piece=▁द्\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9811 size=380 all=39130 active=7780 piece=▁परि\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9445 size=400 all=41055 active=9705 piece=्यों\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9415 min_freq=1429\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8960 size=420 all=42288 active=3222 piece=▁आय\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8386 size=440 all=43529 active=4463 piece=▁दौर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8077 size=460 all=45064 active=5998 piece=▁चल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7730 size=480 all=46422 active=7356 piece=▁सकता\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7442 size=500 all=47949 active=8883 piece=▁चुनाव\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7424 min_freq=1125\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7087 size=520 all=49692 active=4134 piece=▁भारतीय\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6786 size=540 all=51326 active=5768 piece=▁अव\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6635 size=560 all=52202 active=6644 piece=पर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6283 size=580 all=53798 active=8240 piece=▁क्र\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5999 size=600 all=55396 active=9838 piece=भाव\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5982 min_freq=904\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5774 size=620 all=56792 active=4071 piece=▁बो\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5563 size=640 all=58473 active=5752 piece=योग\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5284 size=660 all=59469 active=6748 piece=▁इससे\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5123 size=680 all=60536 active=7815 piece=▁कुमार\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4968 size=700 all=62054 active=9333 piece=ुक\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4952 min_freq=761\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4780 size=720 all=63542 active=4436 piece=वल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4563 size=740 all=64700 active=5594 piece=▁इस्\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 for Unigram\n",
      "\n",
      "▁के: 155\n",
      "।: 137\n",
      "▁में: 135\n",
      "▁है: 134\n",
      "▁की: 100\n",
      "▁को: 74\n",
      "▁का: 64\n",
      "▁और: 64\n",
      "▁से: 62\n",
      "▁ने: 52\n",
      "\n",
      "Top 10 for Bigram\n",
      "\n",
      "('▁है', '।'): 52\n",
      "('▁के', '▁लिए'): 26\n",
      "('▁है', '▁कि'): 17\n",
      "('▁हैं', '।'): 14\n",
      "('▁गया', '▁है'): 11\n",
      "('▁कहा', '▁कि'): 11\n",
      "('▁जन', 'तंत्र'): 11\n",
      "('▁के', '▁साथ'): 10\n",
      "('।', '▁इस'): 9\n",
      "('ता', '▁है'): 8\n",
      "\n",
      "Top 10 for Syllable\n",
      "\n",
      "('▁', 'क'): 572\n",
      "('क', '▁'): 539\n",
      "('ं', '▁'): 410\n",
      "('▁', 'ह'): 302\n",
      "('▁', 'म'): 230\n",
      "('▁', '▁'): 207\n",
      "('न', '▁'): 198\n",
      "('▁', 'स'): 194\n",
      "('त', '▁'): 168\n",
      "('स', '▁'): 168\n",
      "\n",
      "Top 10 for Char\n",
      "\n",
      "('क', 'ए'): 204\n",
      "('अ', 'क'): 180\n",
      "('आ', 'क'): 179\n",
      "('ह', 'ऐ'): 178\n",
      "('क', 'अ'): 172\n",
      "('न', 'ए'): 172\n",
      "('अ', 'ह'): 172\n",
      "('प', 'अ'): 154\n",
      "('ए', 'अं'): 153\n",
      "('र', 'अ'): 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33 size=760 all=65843 active=6737 piece=▁आम\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4313 size=780 all=66568 active=7462 piece=▁आने\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4158 size=800 all=67465 active=8359 piece=▁बंद\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4156 min_freq=659\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4025 size=820 all=68540 active=4417 piece=डे\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3923 size=840 all=69446 active=5323 piece=▁इसमें\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: bpe_model.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: bpe_model.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe_model\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: corpus.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5994 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 298383 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 456 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=39649689\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=153\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 298381 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 298381\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 324920\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1364729 min_freq=1990\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220725 size=20 all=7449 active=2256 piece=▁ल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117261 size=40 all=8844 active=3651 piece=िक\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74160 size=60 all=10391 active=5198 piece=▁थ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58450 size=80 all=12553 active=7360 piece=त्र\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43528 size=100 all=14682 active=9489 piece=▁सं\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=43241 min_freq=4149\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35408 size=120 all=16900 active=3171 piece=▁किया\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28955 size=140 all=18442 active=4713 piece=न्ह\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24411 size=160 all=20405 active=6676 piece=▁राज\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21882 size=180 all=22297 active=8568 piece=▁सर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19406 size=200 all=24144 active=10415 piece=▁चु\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=19305 min_freq=2919\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17837 size=220 all=26375 active=3387 piece=गर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16215 size=240 all=27984 active=4996 piece=▁जी\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14802 size=260 all=29833 active=6845 piece=रो\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13436 size=280 all=31773 active=8785 piece=▁उप\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12360 size=300 all=32981 active=9993 piece=▁पुलिस\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12356 min_freq=1941\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11890 size=320 all=34340 active=2990 piece=▁इसके\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10951 size=340 all=35982 active=4632 piece=चार\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10320 size=360 all=37937 active=6587 piece=▁द्\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9811 size=380 all=39130 active=7780 piece=▁परि\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9445 size=400 all=41055 active=9705 piece=्यों\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9415 min_freq=1429\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8960 size=420 all=42288 active=3222 piece=▁आय\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8386 size=440 all=43529 active=4463 piece=▁दौर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8077 size=460 all=45064 active=5998 piece=▁चल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7730 size=480 all=46422 active=7356 piece=▁सकता\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7442 size=500 all=47949 active=8883 piece=▁चुनाव\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7424 min_freq=1125\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7087 size=520 all=49692 active=4134 piece=▁भारतीय\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6786 size=540 all=51326 active=5768 piece=▁अव\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6635 size=560 all=52202 active=6644 piece=पर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6283 size=580 all=53798 active=8240 piece=▁क्र\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5999 size=600 all=55396 active=9838 piece=भाव\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5982 min_freq=904\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5774 size=620 all=56792 active=4071 piece=▁बो\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5563 size=640 all=58473 active=5752 piece=योग\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5284 size=660 all=59469 active=6748 piece=▁इससे\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5123 size=680 all=60536 active=7815 piece=▁कुमार\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4968 size=700 all=62054 active=9333 piece=ुक\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4952 min_freq=761\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4780 size=720 all=63542 active=4436 piece=वल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4563 size=740 all=64700 active=5594 piece=▁इस्\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4433 size=760 all=65843 active=6737 piece=▁आम\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4313 size=780 all=66568 active=7462 piece=▁आने\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4158 size=800 all=67465 active=8359 piece=▁बंद\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4156 min_freq=659\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4025 size=820 all=68540 active=4417 piece=डे\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3923 size=840 all=69446 active=5323 piece=▁इसमें\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3812 size=860 all=70222 active=6099 piece=▁मीड\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3706 size=880 all=70861 active=6738 piece=▁आयोज\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3630 size=900 all=72163 active=8040 piece=▁आया\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3630 min_freq=588\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3535 size=920 all=72944 active=4381 piece=फ्त\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3458 size=940 all=73699 active=5136 piece=▁कार्यक्रम\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3379 size=960 all=74936 active=6373 piece=▁कोर्ट\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3316 size=980 all=75695 active=7132 piece=मुख\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3250 size=1000 all=76486 active=7923 piece=▁शर्मा\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3246 min_freq=531\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3160 size=1020 all=77662 active=4994 piece=▁रन\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3105 size=1040 all=78887 active=6219 piece=▁एन\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3044 size=1060 all=79393 active=6725 piece=▁चि\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2958 size=1080 all=80142 active=7474 piece=▁मंद\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2893 size=1100 all=80940 active=8272 piece=▁रात\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2889 min_freq=482\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2825 size=1120 all=81895 active=4993 piece=ल्प\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2778 size=1140 all=83181 active=6279 piece=▁युव\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2726 size=1160 all=84012 active=7110 piece=▁बर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2671 size=1180 all=84712 active=7810 piece=▁महीने\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2630 size=1200 all=86080 active=9178 piece=▁जाएगी\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2629 min_freq=432\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2596 size=1220 all=87303 active=5526 piece=▁गिरफ्त\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2543 size=1240 all=88051 active=6274 piece=▁मौके\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2485 size=1260 all=88922 active=7145 piece=बल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2438 size=1280 all=89712 active=7935 piece=▁यही\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2375 size=1300 all=90315 active=8538 piece=ुन\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2372 min_freq=396\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2323 size=1320 all=91618 active=5666 piece=▁“\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2280 size=1340 all=92486 active=6534 piece=▁लगाया\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2240 size=1360 all=93052 active=7100 piece=▁लगातार\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2200 size=1380 all=93957 active=8005 piece=▁शिकायत\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2169 size=1400 all=94750 active=8798 piece=▁भग\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2168 min_freq=366\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2129 size=1420 all=95553 active=5514 piece=▁रखने\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2098 size=1440 all=96282 active=6243 piece=▁देखने\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2057 size=1460 all=97045 active=7006 piece=▁लिखा\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2040 size=1480 all=98031 active=7992 piece=▁लगता\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1995 size=1500 all=98582 active=8543 piece=▁सामान\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1993 min_freq=343\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1968 size=1520 all=99572 active=5914 piece=बंधन\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1942 size=1540 all=100273 active=6615 piece=गम\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1923 size=1560 all=100957 active=7299 piece=▁छोट\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1888 size=1580 all=101747 active=8089 piece=▁शो\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1856 size=1600 all=102552 active=8894 piece=▁पेट\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1855 min_freq=318\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1835 size=1620 all=103434 active=5975 piece=▁नरेंद्र\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1804 size=1640 all=104398 active=6939 piece=खंड\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1780 size=1660 all=105152 active=7693 piece=न्‍\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1765 size=1680 all=105822 active=8363 piece=▁अंग\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1743 size=1700 all=106312 active=8853 piece=▁गौर\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1736 min_freq=299\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1720 size=1720 all=106835 active=5816 piece=▁चाल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1699 size=1740 all=107184 active=6165 piece=▁नागरिक\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1667 size=1760 all=107998 active=6979 piece=▁राय\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1646 size=1780 all=108820 active=7801 piece=▁D\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1631 size=1800 all=109220 active=8201 piece=क्रिया\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1630 min_freq=284\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1611 size=1820 all=110123 active=6329 piece=▁सहयोग\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1593 size=1840 all=110928 active=7134 piece=▁बनी\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: bpe_model.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: bpe_model.vocab\n"
     ]
    }
   ],
   "source": [
    "# BPE with Vocabulary 2000\n",
    "bpe_2000 = train_calculate_and_output(file_path, 'bpe_model', 'bpe', 2000, 'bpe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9de98c",
   "metadata": {},
   "source": [
    "## mBERT and IndicBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeeeabfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abinash/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "def count_frequencies_bert(tokens):\n",
    "    # Count unigram frequencies\n",
    "    unigram_freqs= Counter(tokens)\n",
    "    \n",
    "    bigrams = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "    bigram_freqs = Counter(bigrams)\n",
    "\n",
    "    syllables = []\n",
    "    characters = []\n",
    "    for tok in tokens:\n",
    "        syllables.extend(syllable_convert(tok))\n",
    "        characters.extend(character_conversion(tok))\n",
    "        \n",
    "    \n",
    "    bigram_syllables = [(syllables[i], syllables[i + 1]) for i in range(len(syllables) - 1)]\n",
    "    bigram_characters = [(characters[i], characters[i + 1]) for i in range(len(characters) - 1)]\n",
    "    \n",
    "    syllable_freqs= Counter(bigram_syllables)\n",
    "    char_freqs= Counter(bigram_characters)\n",
    "\n",
    "    return unigram_freqs, bigram_freqs, syllable_freqs, char_freqs\n",
    "\n",
    "def trained_mBERT_tokenizer():\n",
    "    # Load mBERT tokenizer\n",
    "    return BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f327f66",
   "metadata": {},
   "source": [
    "## mBERT for 1k vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c584f147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 for Unigram\n",
      "\n",
      "के: 22\n",
      "में: 21\n",
      "।: 19\n",
      "है: 15\n",
      "##प: 15\n",
      "आ: 13\n",
      "##र: 13\n",
      "ब: 13\n",
      "को: 13\n",
      "##ी: 12\n",
      "\n",
      "Top 10 for Bigram\n",
      "\n",
      "('आ', '##प'): 7\n",
      "('ख', '##ोज'): 7\n",
      "('है', '।'): 6\n",
      "('है', 'कि'): 4\n",
      "('##ोज', 'शब्द'): 4\n",
      "('##ों', 'की'): 3\n",
      "('##्', '##प'): 3\n",
      "('को', 'ब'): 3\n",
      "('##रल', '##ैंड'): 3\n",
      "('वि', '##के'): 3\n",
      "\n",
      "Top 10 for Syllable\n",
      "\n",
      "('#', '#'): 580\n",
      "('#', '्'): 52\n",
      "('#', 'ि'): 39\n",
      "('क', '#'): 35\n",
      "('्', '#'): 34\n",
      "('#', 'क'): 32\n",
      "('#', 'े'): 25\n",
      "('ं', '#'): 23\n",
      "('म', 'ं'): 21\n",
      "('ि', '#'): 20\n",
      "\n",
      "Top 10 for Char\n",
      "\n",
      "('आ', 'क'): 31\n",
      "('क', 'ए'): 31\n",
      "('अ', 'क'): 25\n",
      "('र', 'अ'): 25\n",
      "('ह', 'ऐ'): 25\n",
      "('क', 'अ'): 23\n",
      "('अ', 'न'): 23\n",
      "('य', 'आ'): 22\n",
      "('म', 'ए'): 22\n",
      "('ए', 'अं'): 22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mbert_tokenizer = trained_mBERT_tokenizer() \n",
    "\n",
    "# Tokenization with mBERT \n",
    "text_encoding = mbert_tokenizer.encode_plus(\n",
    "    data, \n",
    "    max_length=1000, \n",
    "    truncation=True, \n",
    "    padding=True\n",
    ")\n",
    "mbert_tokens = mbert_tokenizer.convert_ids_to_tokens(text_encoding['input_ids'])\n",
    "\n",
    "\n",
    "token_frequencies = count_frequencies_bert(mbert_tokens)\n",
    "unigram_counts, bigram_counts, syllable_counts, char_counts = token_frequencies \n",
    "\n",
    "\n",
    "frequency_list = [unigram_counts, bigram_counts, syllable_counts, char_counts]\n",
    "\n",
    "\n",
    "write_to_file_and_print_top_k(\n",
    "    model_name=\"mBERT\", \n",
    "    vocab=\"1000\", \n",
    "    k=10, \n",
    "    frequencies=frequency_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bad1ec",
   "metadata": {},
   "source": [
    "## mBERT for 2k vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8100aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 for Unigram\n",
      "\n",
      "।: 37\n",
      "में: 36\n",
      "के: 34\n",
      "है: 29\n",
      "##र: 26\n",
      "की: 25\n",
      "ब: 25\n",
      "##ी: 24\n",
      "##ा: 23\n",
      "ने: 22\n",
      "\n",
      "Top 10 for Bigram\n",
      "\n",
      "('है', '।'): 12\n",
      "('आ', '##प'): 7\n",
      "('ख', '##ोज'): 7\n",
      "('हैं', '।'): 6\n",
      "('है', 'कि'): 5\n",
      "('के', 'लिए'): 5\n",
      "('त', '##ु'): 5\n",
      "('स', '##्व'): 4\n",
      "('##ोज', 'शब्द'): 4\n",
      "('म', '##ु'): 4\n",
      "\n",
      "Top 10 for Syllable\n",
      "\n",
      "('#', '#'): 1161\n",
      "('#', '्'): 87\n",
      "('क', '#'): 68\n",
      "('#', 'ि'): 67\n",
      "('#', 'क'): 58\n",
      "('्', '#'): 53\n",
      "('#', 'ा'): 49\n",
      "('ं', '#'): 41\n",
      "('म', 'ं'): 37\n",
      "('#', 'े'): 36\n",
      "\n",
      "Top 10 for Char\n",
      "\n",
      "('आ', 'क'): 50\n",
      "('न', 'ए'): 47\n",
      "('क', 'ए'): 45\n",
      "('अ', 'क'): 44\n",
      "('ह', 'ऐ'): 44\n",
      "('य', 'आ'): 42\n",
      "('क', 'अ'): 40\n",
      "('र', 'अ'): 40\n",
      "('म', 'ए'): 40\n",
      "('ए', 'अं'): 39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mbert_tokenizer = trained_mBERT_tokenizer() \n",
    "\n",
    "# Tokenization with mBERT \n",
    "text_encoding = mbert_tokenizer.encode_plus(\n",
    "    data, \n",
    "    max_length=2000, \n",
    "    truncation=True, \n",
    "    padding=True\n",
    ")\n",
    "mbert_tokens = mbert_tokenizer.convert_ids_to_tokens(text_encoding['input_ids'])\n",
    "\n",
    "\n",
    "token_frequencies = count_frequencies_bert(mbert_tokens)\n",
    "unigram_counts, bigram_counts, syllable_counts, char_counts = token_frequencies \n",
    "\n",
    "\n",
    "frequency_list = [unigram_counts, bigram_counts, syllable_counts, char_counts]\n",
    "\n",
    "\n",
    "write_to_file_and_print_top_k(\n",
    "    model_name=\"mBERT\", \n",
    "    vocab=\"2000\",  #'vocab' might mean tokenization limit here\n",
    "    k=10, \n",
    "    frequencies=frequency_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf22825",
   "metadata": {},
   "source": [
    "## IndicBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57ffa722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import collections\n",
    "\n",
    "def load_indic_bert_tokenizer(): \n",
    "    \"Loads a pre-trained IndicBERT tokenizer\"\n",
    "    return transformers.AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "def process_with_indicbert(data, max_length):\n",
    "    \"\"\"Processes data with IndicBERT and saves token frequencies.\n",
    "\n",
    "    Args:\n",
    "       data (str): Input text data. \n",
    "       max_length (int): Maximum length for tokenization. \n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = load_indic_bert_tokenizer()\n",
    "    tokens = tokenizer(data, max_length=max_length, truncation=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens['input_ids'])\n",
    "\n",
    "    frequencies = count_frequencies_bert(tokens)  \n",
    "    frequency_list = list(frequencies) \n",
    "    write_to_file_and_print_top_k(\n",
    "        model_name=\"IndicBERT\",\n",
    "        vocab=str(max_length),  # Using max_length to represent vocab here\n",
    "        k=10, \n",
    "        frequencies=frequency_list\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4785db",
   "metadata": {},
   "source": [
    "### IndicBERT for 1000 vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d347dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 for Unigram\n",
      "\n",
      "▁क: 71\n",
      "▁ह: 38\n",
      "▁म: 26\n",
      "।: 24\n",
      "य: 24\n",
      "क: 23\n",
      "▁: 21\n",
      "त: 19\n",
      "न: 16\n",
      "▁और: 16\n",
      "\n",
      "Top 10 for Bigram\n",
      "\n",
      "('▁ह', '।'): 12\n",
      "('▁क', '▁'): 6\n",
      "('न', '▁क'): 6\n",
      "('त', '▁ह'): 6\n",
      "('▁ल', 'ए'): 5\n",
      "('▁आप', 'क'): 4\n",
      "('▁द', 'य'): 4\n",
      "('य', '▁ह'): 4\n",
      "('▁ह', '▁क'): 4\n",
      "('ष', 'त'): 4\n",
      "\n",
      "Top 10 for Syllable\n",
      "\n",
      "('▁', '▁'): 445\n",
      "('।', '▁'): 24\n",
      "('▁', 'आ'): 22\n",
      "('▁', 'ए'): 21\n",
      "('▁', '।'): 20\n",
      "('आ', '▁'): 19\n",
      "('इ', '▁'): 19\n",
      "('▁', 'अ'): 19\n",
      "('अ', '▁'): 18\n",
      "('▁', 'इ'): 17\n",
      "\n",
      "Top 10 for Char\n",
      "\n",
      "('क', 'अ'): 49\n",
      "('प', 'अ'): 45\n",
      "('र', 'अ'): 44\n",
      "('अ', 'क'): 43\n",
      "('अ', 'प'): 40\n",
      "('अ', 'र'): 39\n",
      "('स', 'अ'): 35\n",
      "('ज', 'अ'): 30\n",
      "('अ', 'स'): 27\n",
      "('ब', 'अ'): 25\n"
     ]
    }
   ],
   "source": [
    "process_with_indicbert(data, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806947e",
   "metadata": {},
   "source": [
    "### IndicBERT for 2000 vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adaf6203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 for Unigram\n",
      "\n",
      "▁क: 132\n",
      "▁ह: 77\n",
      "य: 55\n",
      "▁म: 51\n",
      "।: 48\n",
      "▁न: 44\n",
      "क: 42\n",
      "▁: 40\n",
      "न: 38\n",
      "त: 37\n",
      "\n",
      "Top 10 for Bigram\n",
      "\n",
      "('▁ह', '।'): 26\n",
      "('त', '▁ह'): 11\n",
      "('▁ल', 'ए'): 10\n",
      "('▁न', 'ह'): 10\n",
      "('य', '▁क'): 8\n",
      "('।', '▁इस'): 8\n",
      "('▁रह', '▁ह'): 8\n",
      "('▁क', '▁ल'): 8\n",
      "('▁क', '▁'): 7\n",
      "('▁द', 'य'): 6\n",
      "\n",
      "Top 10 for Syllable\n",
      "\n",
      "('▁', '▁'): 918\n",
      "('।', '▁'): 48\n",
      "('▁', 'आ'): 41\n",
      "('▁', '।'): 41\n",
      "('▁', 'इ'): 38\n",
      "('▁', 'ए'): 38\n",
      "('इ', '▁'): 37\n",
      "('आ', '▁'): 36\n",
      "('ए', '▁'): 35\n",
      "('▁', 'अ'): 34\n",
      "\n",
      "Top 10 for Char\n",
      "\n",
      "('क', 'अ'): 99\n",
      "('अ', 'क'): 84\n",
      "('प', 'अ'): 83\n",
      "('र', 'अ'): 77\n",
      "('स', 'अ'): 71\n",
      "('अ', 'प'): 70\n",
      "('अ', 'र'): 68\n",
      "('अ', 'स'): 58\n",
      "('व', 'अ'): 56\n",
      "('ज', 'अ'): 55\n"
     ]
    }
   ],
   "source": [
    "process_with_indicbert(data, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed8427",
   "metadata": {},
   "source": [
    "## WhiteSpace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d75c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 for Unigram\n",
      "\n",
      "के: 150\n",
      "में: 135\n",
      "की: 94\n",
      "है: 81\n",
      "को: 72\n",
      "और: 64\n",
      "से: 56\n",
      "का: 55\n",
      "है।: 52\n",
      "ने: 49\n",
      "\n",
      "Top 10 for Bigram\n",
      "\n",
      "('के', 'लिए'): 26\n",
      "('है', 'कि'): 17\n",
      "('कहा', 'कि'): 11\n",
      "('के', 'साथ'): 9\n",
      "('ने', 'कहा'): 8\n",
      "('हैं', 'और'): 7\n",
      "('बताया', 'कि'): 7\n",
      "('के', 'बाद'): 7\n",
      "('गया', 'है।'): 7\n",
      "('करने', 'के'): 7\n",
      "\n",
      "Top 10 for Syllable\n",
      "\n",
      "('म', 'ं'): 146\n",
      "('ं', 'क'): 101\n",
      "('ह', 'ं'): 70\n",
      "('क', 'स'): 58\n",
      "('ह', '।'): 53\n",
      "('क', 'ल'): 52\n",
      "('त', 'ह'): 39\n",
      "('प', 'र'): 36\n",
      "('र', 'क'): 36\n",
      "('क', 'क'): 35\n",
      "\n",
      "Top 10 for Char\n",
      "\n",
      "('र', 'अ'): 227\n",
      "('क', 'अ'): 224\n",
      "('अ', 'न'): 217\n",
      "('अ', 'क'): 210\n",
      "('क', 'ए'): 206\n",
      "('स', 'अ'): 202\n",
      "('अ', 'ह'): 201\n",
      "('अ', 'र'): 192\n",
      "('प', 'अ'): 184\n",
      "('ए', 'अं'): 182\n"
     ]
    }
   ],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    stripped_text = text.strip()\n",
    "    if not stripped_text:  \n",
    "        return []\n",
    "\n",
    "    return stripped_text.split()  \n",
    "\n",
    "tokens = whitespace_tokenize(data)\n",
    "# print(tokens)\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies_bert(tokens)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "write_to_file_and_print_top_k(model_name=\"WhiteSpace\", vocab=\"\", k=10, frequencies=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63572dc0",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Assume that the set of tokens from Question 3 is the ground truth set. For each\n",
    "tokenizer in Question 4, find the precision, recall and F-score for the 25 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc68b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अलीगढ़ मुस्लिम विश्वविद्यालय में कई शिक्षकों ने मौन जुलूस निकाला।एचजीएस धालीवाल आईपीएस दिल्ली में ही पदस्थ हैं।बलदेव और सूबेदार मेजर ब्रह्मू ने कहा कि सीताराम भारद्वाज हमारे पड़ोसी हैं।हर साल मई में भी थोड़ी बहुत बारिश हो जाती थी लेकिन इस बार बादल दूर-दूर ही रहे। इसका असर किसानाें पर साफ दिख रहा है।ऐसे में सरकार की प्राथमिकता रहेगी कि यह जमीन बोर्ड-निगमों को ही बेची जाए ताकि वे यहां अपनी गतिविधियों को बढ़ावा दे सकें।इस सूरत में जो भी नीति प्रमाणित गुणवत्ता वाले शैक्षणिक संस्थानों के अभाव से जूझने का प्रयास न करे वह अप्रासंगिक बनने को अभिशप्त रहेगी।राष्ट्रीय सुरक्षा से जुड़े मुद्दों पर सरकार के साथ एकजुट होने के लिए विपक्ष से अपील करते हुए, ।कंपनियां अब यूसी सेवा की उपयोगिता समझने लगी हैं इसलिए अब इसमें निवेश भी कर रही हैं।31 ओवर के बाद भारत 243/3, जीत के लिए 65 रन चाहिए आपात्काल मे धेेैर्य , अभ्युदय मे क्षमा , सदन मे वाक्पटुता , युद्ध के समय बहादुरी , यशमे अभिरूचि , ज्ञान का व्यसन ये सब चीजे महापुरूषोंमे नैसर्गिक रूपसे पायी जाती हैं । उपराज्यपाल ने कमीशन ऑफ एनक्वायरी एक्ट का हवाला देते हुए कहा है कि ये कानून सिर्फ केंद्र और राज्य सरकारों को ही जांच आयोग बनाने की इजाजत देता है। हमने सोचा कि कोई चीज नीचे गिर गई है, कोई पटाखा फूटा है या जनरेटर में विस्फोट हो गया है। सिसोदिया के हाथ, आंखों के पास और उनके सरकारी वाहन पर भी स्याही के निशान थे। उत्तराखण्ड आयुर्वेद विश्व विघालय के कालेजों में फीस बढ़ोत्तरी एंव छात्रों के उत्पीड़न के विरोध में उत्तराखण्ड क्रांतिदल के कार्यकताओं ने आज घंटाघर स्थित इंद्रमणी बडोनी की प्रतिमा के समीप धरना प्रदर्शन कर राज्यपाल के नाम ज्ञापन प्रेषित किया। बंगाल सरकार ने जलपाईगुड़ी के रिक्शा चालक की बेटी स्वप्ना के लिए 10 लाख रुपये का पुरस्कार और सरकारी नौकरी की घोषणा की। सिखा के कुल्हे खोल के मैंने उसके पिछवाड़े के छेद पर ये झाग को लगा दिया. गांड के छेद को एकदम चिकना कर के मैंने लंड के ऊपर भी झाग बनाया. 20/08/2014 मुख्य अभियंता, धसान केन कछार सागर के अन्तर्गत गुण नियंत्रण के कार्यों को गति देने हेतु मुख्यालय परिवर्तन। मैंने कभी भी तुम्हारा मित्र के अलावा किसी और तरीके से स्पर्श किया है? अब भगवान को न याद किया तो शायद सिनेमा चौपट हो जाये सो एक भजन भी लाजिमी है। 22:00 IST 18 ओवर के बाद ऑस्ट्रेलिया का स्कोर 111/5, जीत के लिए 12 गेंदों में 16 रन चाहिए। 19वीं शताब्दी के आरंभ तक बहुत से लोगों का यही मानना था कि प्राचीन पत्थर, पुरानी बीमारियों को दूर करने और समस्याओं को समाप्त करने में सक्षम होते हैं। उनकी इस रचना का विमोचन सोमवार को राजभवन में आचार्य देवव्रत ने किया। केवल छिंदवाड़ा सीट कांग्रेस के खाते में आई है। ऐसे लोग भ्रांति फैला कर उसका लाभ उठाना चाहते हैं। यही है दूषित चेतना को परिष्कृत करके एक ऐसी पवित्र चेतना में परिवर्तित कर देना।\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\n",
    "sentences_file = 'text.txt' \n",
    "\n",
    "with open(sentences_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line !=\"\\n\":\n",
    "            temp = line.strip()\n",
    "            sentences = sentences+temp[3:]\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23537e76",
   "metadata": {},
   "source": [
    "## Functions for Precision, Recall and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f730c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_leading_underscore(token):\n",
    "    return re.sub(r\"^_\", \"\", token)  # Remove a single leading underscore\n",
    "\n",
    "def clean_and_create_set(tokens):\n",
    "    \"\"\"Removes leading underscores from tokens and creates a set.\"\"\"\n",
    "    cleaned_tokens = [remove_leading_underscore(token) for token in tokens]\n",
    "    return set(cleaned_tokens)\n",
    "  \n",
    "def calculate_metrics(true_tokens, pred_tokens):\n",
    "\n",
    "#     true_set = set(true_tokens)\n",
    "#     pred_set = set(pred_tokens)\n",
    "    \n",
    "    true_set = clean_and_create_set(true_tokens)\n",
    "    pred_set = clean_and_create_set(pred_tokens)\n",
    "\n",
    "\n",
    "    true_positives = len(true_set & pred_set)  \n",
    "\n",
    "    precision = true_positives / (len(pred_set) or 1)  \n",
    "    recall = true_positives / (len(true_set) or 1)   # Avoid division by zero\n",
    "\n",
    "    f1_score = (2 * precision * recall) / (precision + recall or 1)  \n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_and_print_metrics(true_tokens, pred_tokens):\n",
    "    \"\"\"Calculates precision, recall, F1-score, and prints the results.\"\"\"\n",
    "    precision, recall, f1 = calculate_metrics(true_tokens, pred_tokens)\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e5403cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of True Tokens: ['अलीगढ़ मुस्लिम विश्वविद्यालय में', 'कई', 'शिक्षकों ने', 'मौन', 'जुलूस', 'निकाला।\\n\\n\\nएचजीएस धालीवाल आईपीएस', 'दिल्ली में ही', 'पदस्थ हैं।\\n\\n\\nबलदेव', 'और', 'सूबेदार मेजर ब्रह्मू ने', 'कहा कि', 'सीताराम भारद्वाज', 'हमारे', 'पड़ोसी हैं।\\n\\n\\nहर साल', 'मई में', 'भी', 'थोड़ी बहुत', 'बारिश', 'हो जाती थी', 'लेकिन', 'इस बार', 'बादल', 'दूर-दूर', 'ही रहे।', 'इसका', 'असर', 'किसानाें पर', 'साफ', 'दिख रहा है।\\n\\n\\n ऐसे में', 'सरकार की', 'प्राथमिकता', 'रहेगी', 'कि', 'यह', 'जमीन', 'बोर्ड-निगमों को ही', 'बेची', 'जाए', 'ताकि', 'वे', 'यहां', 'अपनी', 'गतिविधियों को', 'बढ़ावा', 'दे सकें।\\n\\n\\nइस सूरत में', 'जो भी', 'नीति', 'प्रमाणित गुणवत्ता', 'वाले', 'शैक्षणिक संस्थानों के', 'अभाव से', 'जूझने का', 'प्रयास', 'न', 'करे', 'वह', 'अप्रासंगिक', 'बनने को', 'अभिशप्त', 'रहेगी।\\n\\n\\nराष्ट्रीय सुरक्षा से', 'जुड़े', 'मुद्दों पर', 'सरकार', 'के साथ', 'एकजुट', 'होने के लिए', 'विपक्ष से', 'अपील', 'करते हुए', '।\\n\\n\\nकंपनियां', 'अब', 'यूसी सेवा की', 'उपयोगिता', 'समझने लगी हैं', 'इसलिए', 'अब', 'इसमें', 'निवेश भी', 'कर रही हैं।\\n\\n\\n31 ओवर के', 'बाद', 'भारत', '243/3', 'जीत के लिए', '65 रन', 'चाहिए\\n\\n\\nआपात्काल मे धेेैर्य', 'अभ्युदय मे क्षमा', 'सदन मे वाक्पटुता', 'युद्ध के समय बहादुरी', 'यशमे अभिरूचि', 'ज्ञान का व्यसन', 'ये', 'सब चीजे', 'महापुरूषोंमे', 'नैसर्गिक', 'रूपसे', 'पायी जाती हैं ।\\n\\n\\nउपराज्यपाल ने', 'कमीशन ऑफ एनक्वायरी एक्ट का', 'हवाला देते हुए', 'कहा है कि', 'ये', 'कानून', 'सिर्फ', 'केंद्र', 'और', 'राज्य सरकारों', 'को ही', 'जांच आयोग', 'बनाने की', 'इजाजत', 'देता है।\\n\\n\\nहमने', 'सोचा कि', 'कोई चीज', 'नीचे', 'गिर गई है', 'कोई', 'पटाखा', 'फूटा है', 'या', 'जनरेटर में', 'विस्फोट', 'हो गया है।\\n\\n\\nसिसोदिया के', 'हाथ', 'आंखों', 'के पास', 'और', 'उनके', 'सरकारी वाहन पर', 'भी', 'स्याही के', 'निशान थे।\\n\\n\\nउत्तराखण्ड आयुर्वेद विश्व विघालय के', 'कालेजों में', 'फीस', 'बढ़ोत्तरी', 'एंव', 'छात्रों के', 'उत्पीड़न के', 'विरोध में', 'उत्तराखण्ड क्रांतिदल के', 'कार्यकताओं ने', 'आज', 'घंटाघर स्थित', 'इंद्रमणी बडोनी की', 'प्रतिमा के', 'समीप', 'धरना प्रदर्शन कर', 'राज्यपाल के', 'नाम', 'ज्ञापन', 'प्रेषित किया।\\n\\n\\nबंगाल सरकार ने', 'जलपाईगुड़ी के', 'रिक्शा चालक की', 'बेटी', 'स्वप्ना के लिए', '10 लाख रुपये का', 'पुरस्कार', 'और', 'सरकारी नौकरी की', 'घोषणा की।\\n\\n\\nसिखा के', 'कुल्हे', 'खोल के', 'मैंने', 'उसके', 'पिछवाड़े के', 'छेद पर', 'ये', 'झाग को', 'लगा दिया', 'गांड के', 'छेद को', 'एकदम', 'चिकना कर के', 'मैंने', 'लंड के', 'ऊपर', 'भी', 'झाग बनाया.\\n\\n\\n20/08/2014', 'मुख्य अभियंता', 'धसान केन कछार सागर के', 'अन्तर्गत', 'गुण नियंत्रण के', 'कार्यों को', 'गति', 'देने हेतु', 'मुख्यालय', 'परिवर्तन।\\n\\n\\nमैंने', 'कभी भी', 'तुम्हारा', 'मित्र', 'के अलावा', 'किसी और', 'तरीके से', 'स्पर्श किया है?\\n\\n\\nअब', 'भगवान को', 'न', 'याद किया', 'तो', 'शायद', 'सिनेमा', 'चौपट हो जाये', 'सो', 'एक भजन', 'भी', 'लाजिमी है।\\n\\n\\n22:00 IST', '18 ओवर के', 'बाद', 'ऑस्ट्रेलिया का', 'स्कोर', '111/5', 'जीत के लिए', '12 गेंदों में', '16 रन', 'चाहिए।\\n\\n\\n19वीं शताब्दी के', 'आरंभ तक', 'बहुत से', 'लोगों का', 'यही', 'मानना था', 'कि', 'प्राचीन पत्थर', 'पुरानी', 'बीमारियों को', 'दूर करने', 'और', 'समस्याओं को', 'समाप्त करने में', 'सक्षम होते हैं।\\n\\n\\nउनकी', 'इस', 'रचना का', 'विमोचन', 'सोमवार को', 'राजभवन में', 'आचार्य देवव्रत ने', 'किया।\\n\\n\\nकेवल', 'छिंदवाड़ा सीट', 'कांग्रेस के', 'खाते में', 'आई है।\\n\\n\\nऐसे', 'लोग', 'भ्रांति', 'फैला कर', 'उसका', 'लाभ', 'उठाना चाहते हैं।\\n\\nयही है', 'दूषित', 'चेतना को', 'परिष्कृत', 'करके', 'एक', 'ऐसी', 'पवित्र', 'चेतना में', 'परिवर्तित', 'कर देना।']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'truth.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    tokens = file.read().split(',')\n",
    "\n",
    "true_tokens = [word.strip() for word in tokens]\n",
    "\n",
    "print(\"List of True Tokens:\", true_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c8576",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1412019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.01\n",
      "Recall: 0.02\n",
      "F1-Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "pred_tokens = unigram_1000.encode(sentences, out_type=str)\n",
    "calculate_and_print_metrics(true_tokens, pred_tokens) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17a12e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.00\n",
      "Recall: 0.01\n",
      "F1-Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Unigram Evaluation\n",
    "pred_tokens = unigram_2000.encode(sentences, out_type=str)\n",
    "calculate_and_print_metrics(true_tokens, pred_tokens) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392fa7da",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf8831f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.01\n",
      "Recall: 0.03\n",
      "F1-Score: 0.02\n"
     ]
    }
   ],
   "source": [
    "# BPE Evaluation \n",
    "pred_tokens = bpe_1000.encode(sentences, out_type=str)\n",
    "calculate_and_print_metrics(true_tokens, pred_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f061970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.01\n",
      "Recall: 0.01\n",
      "F1-Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "# BPE Evaluation \n",
    "pred_tokens = bpe_2000.encode(sentences, out_type=str)\n",
    "calculate_and_print_metrics(true_tokens, pred_tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb2e46",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8321fd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.09\n",
      "Recall: 0.16\n",
      "F1-Score: 0.11\n"
     ]
    }
   ],
   "source": [
    "tokenizer = trained_mBERT_tokenizer()\n",
    "mBert_encodings = tokenizer.encode_plus(sentences, max_length=1000, truncation=True, padding=True)\n",
    "pred_tokens = tokenizer.convert_ids_to_tokens(mBert_encodings['input_ids'])\n",
    "\n",
    "calculate_and_print_metrics(true_tokens, pred_tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833d3f8",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for IndicBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c4bce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'अ', '##ली', '##गढ', '##़', 'मुस्लिम', 'विश्वविद्यालय', 'में', 'कई', 'श', '##िक', '##्ष', '##कों', 'ने', 'म', '##ौ', '##न', 'जुल', '##ूस', 'न', '##िका', '##ला', '।', 'ए', '##च', '##जी', '##ए', '##स', 'ध', '##ाली', '##वाल', 'आई', '##पी', '##ए', '##स', 'दिल्ली', 'में', 'ही', 'पद', '##स', '##्थ', 'हैं', '।', 'ब', '##ल', '##देव', 'और', 'स', '##ू', '##बे', '##दार', 'मे', '##ज', '##र', 'ब', '##्रह', '##्म', '##ू', 'ने', 'कहा', 'कि', 'सी', '##ता', '##रा', '##म', 'भ', '##ार', '##द', '##्व', '##ाज', 'हम', '##ारे', 'प', '##ड़', '##ो', '##सी', 'हैं', '।', 'हर', 'साल', 'मई', 'में', 'भी', 'थ', '##ोड़', '##ी', 'बहुत', 'बार', '##िश', 'हो', 'जाती', 'थी', 'लेकिन', 'इस', 'बार', 'बाद', '##ल', 'दूर', '-', 'दूर', 'ही', 'रहे', '।', 'इसका', 'अ', '##सर', 'कि', '##सा', '##ना', '##ें', 'पर', 'स', '##ा', '##फ', 'द', '##ि', '##ख', 'रहा', 'है', '।', 'ऐसे', 'में', 'सरकार', 'की', 'प्राथमिक', '##ता', 'रहे', '##गी', 'कि', 'यह', 'ज', '##मीन', 'ब', '##ोर्ड', '-', 'न', '##ि', '##गम', '##ों', 'को', 'ही', 'ब', '##े', '##ची', 'जा', '##ए', 'त', '##ा', '##कि', 'वे', 'यहां', 'अपनी', 'ग', '##ति', '##वि', '##धि', '##यों', 'को', 'ब', '##ढ़', '##ावा', 'दे', 'स', '##के', '##ं', '।', 'इस', 'स', '##ूर', '##त', 'में', 'जो', 'भी', 'न', '##ीत', '##ि', 'प्रमाण', '##ित', 'ग', '##ुण', '##वत', '##्ता', 'वाले', 'श', '##ैक', '##्षण', '##िक', 'संस्थान', '##ों', 'के', 'अ', '##भा', '##व', 'से', 'ज', '##ू', '##झ', '##ने', 'का', 'प', '##्र', '##यास', 'न', 'कर', '##े', 'वह', 'अ', '##प', '##्रा', '##स', '##ंग', '##िक', 'बन', '##ने', 'को', 'अ', '##भ', '##िश', '##प्त', 'रहे', '##गी', '।', 'राष्ट्रीय', 'सुरक्षा', 'से', 'ज', '##ु', '##ड़े', 'म', '##ु', '##द', '##्द', '##ों', 'पर', 'सरकार', 'के', 'साथ', 'एक', '##ज', '##ुट', 'होने', 'के', 'लिए', 'वि', '##प', '##क्ष', 'से', 'अ', '##पी', '##ल', 'करते', 'हुए', ',', '।', 'क', '##ं', '##पन', '##िया', '##ं', 'अब', 'य', '##ूस', '##ी', 'सेवा', 'की', 'उपयोग', '##िता', 'स', '##म', '##झ', '##ने', 'ल', '##गी', 'हैं', 'इसलिए', 'अब', 'इसमें', 'न', '##िव', '##ेश', 'भी', 'कर', 'रही', 'हैं', '।', '31', 'ओ', '##वर', 'के', 'बाद', 'भारत', '243', '/', '3', ',', 'जी', '##त', 'के', 'लिए', '65', 'र', '##न', 'चाहिए', 'आ', '##पा', '##त', '##्क', '##ाल', 'मे', 'ध', '##े', '##े', '##ैर', '##्य', ',', 'अ', '##भ', '##्य', '##ु', '##दय', 'मे', 'क', '##्ष', '##मा', ',', 'स', '##दन', 'मे', 'वा', '##क', '##्', '##प', '##ट', '##ुत', '##ा', ',', 'युद्ध', 'के', 'समय', 'ब', '##हा', '##द', '##ुरी', ',', 'य', '##श', '##म', '##े', 'अ', '##भ', '##िर', '##ू', '##च', '##ि', ',', 'ज्ञान', 'का', 'व', '##्य', '##सन', 'ये', 'सब', 'च', '##ी', '##जे', 'म', '##हा', '##पुर', '##ू', '##ष', '##ों', '##म', '##े', 'नै', '##सर', '##्ग', '##िक', 'रूप', '##से', 'प', '##ाय', '##ी', 'जाती', 'हैं', '।', 'उ', '##पर', '##ाज', '##्य', '##पा', '##ल', 'ने', 'कमी', '##शन', 'ऑफ', 'एन', '##क', '##्व', '##ाय', '##री', 'एक', '##्ट', 'का', 'ह', '##वाल', '##ा', 'देते', 'हुए', 'कहा', 'है', 'कि', 'ये', 'का', '##न', '##ून', 'स', '##िर', '##्फ', 'केंद्र', 'और', 'राज्य', 'सरकार', '##ों', 'को', 'ही', 'जा', '##ंच', 'आयोग', 'बनाने', 'की', 'इ', '##जा', '##ज', '##त', 'देता', 'है', '।', 'हम', '##ने', 'स', '##ो', '##चा', 'कि', 'कोई', 'च', '##ी', '##ज', 'नीचे', 'ग', '##िर', 'गई', 'है', ',', 'कोई', 'प', '##टा', '##खा', 'फ', '##ूट', '##ा', 'है', 'या', 'ज', '##न', '##रे', '##टर', 'में', 'वि', '##स', '##्फ', '##ोट', 'हो', 'गया', 'है', '।', 'स', '##िस', '##ो', '##द', '##िया', 'के', 'हा', '##थ', ',', 'आ', '##ं', '##ख', '##ों', 'के', 'पास', 'और', 'उनके', 'सरकारी', 'वा', '##हन', 'पर', 'भी', 'स', '##्या', '##ही', 'के', 'न', '##िशा', '##न', 'थे', '।', 'उत्तराखण्ड', 'आय', '##ुर', '##्वे', '##द', 'विश्व', 'वि', '##घ', '##ालय', 'के', 'काल', '##ेज', '##ों', 'में', 'फ', '##ी', '##स', 'ब', '##ढ़', '##ो', '##त', '##्त', '##री', 'ए', '##ं', '##व', 'छ', '##ात्र', '##ों', 'के', 'उ', '##त', '##्', '##पी', '##ड़', '##न', 'के', 'वि', '##रो', '##ध', 'में', 'उत्तराखण्ड', 'क', '##्रा', '##ंत', '##ि', '##दल', 'के', 'कार्य', '##क', '##ता', '##ओं', 'ने', 'आज', 'घ', '##ंट', '##ा', '##घ', '##र', 'स्थित', 'इ', '##ंद', '##्रम', '##णी', 'ब', '##ड', '##ोन', '##ी', 'की', 'प्रति', '##मा', 'के', 'स', '##मी', '##प', 'ध', '##र', '##ना', 'प्रदर्शन', 'कर', 'राज्य', '##पा', '##ल', 'के', 'नाम', 'ज', '##्', '##ञ', '##ाप', '##न', 'प', '##्र', '##े', '##षित', 'किया', '।', 'ब', '##ंगा', '##ल', 'सरकार', 'ने', 'जल', '##पा', '##ई', '##गु', '##ड़ी', 'के', 'र', '##िक', '##्श', '##ा', 'च', '##ाल', '##क', 'की', 'ब', '##ेट', '##ी', 'स', '##्व', '##प', '##्न', '##ा', 'के', 'लिए', '10', 'लाख', 'रुप', '##ये', 'का', 'पुरस्कार', 'और', 'सरकारी', 'न', '##ौ', '##कर', '##ी', 'की', 'घोषणा', 'की', '।', 'स', '##िखा', 'के', 'कुल', '##्', '##हे', 'ख', '##ोल', 'के', 'म', '##ैं', '##ने', 'उसके', 'प', '##ि', '##छ', '##वा', '##ड़े', 'के', 'छ', '##ेद', 'पर', 'ये', 'झ', '##ाग', 'को', 'लगा', 'दिया', '.', 'गां', '##ड', 'के', 'छ', '##ेद', 'को', 'एक', '##द', '##म', 'च', '##िक', '##ना', 'कर', 'के', 'म', '##ैं', '##ने', 'ल', '##ंड', 'के', 'ऊपर', 'भी', 'झ', '##ाग', 'बनाया', '.', '20', '/', '08', '/', '2014', 'मुख्य', 'अ', '##भ', '##िय', '##ंत', '##ा', ',', 'ध', '##सा', '##न', 'के', '##न', 'क', '##छ', '##ार', 'स', '##ाग', '##र', 'के', 'अन्तर्गत', 'ग', '##ुण', 'न', '##िय', '##ंत्रण', 'के', 'कार्य', '##ों', 'को', 'ग', '##ति', 'देने', 'हे', '##तु', 'मुख्य', '##ालय', 'परिवर्तन', '।', 'म', '##ैं', '##ने', 'कभी', 'भी', 'त', '##ु', '##म्', '##हार', '##ा', 'म', '##ित्र', 'के', 'अलावा', 'किसी', 'और', 'तर', '##ी', '##के', 'से', 'स', '##्', '##पर', '##्श', 'किया', 'है', '?', 'अब', 'भगवान', 'को', 'न', 'या', '##द', 'किया', 'तो', 'श', '##ाय', '##द', 'स', '##िने', '##मा', 'च', '##ौ', '##प', '##ट', 'हो', 'जा', '##ये', 'स', '##ो', 'एक', 'भ', '##जन', 'भी', 'ला', '##ज', '##िम', '##ी', 'है', '।', '22', ':', '00', 'IS', '##T', '18', 'ओ', '##वर', 'के', 'बाद', 'ऑ', '##स्ट', '##्र', '##ेल', '##िया', 'का', 'स', '##्क', '##ोर', '111', '/', '5', ',', 'जी', '##त', 'के', 'लिए', '12', 'ग', '##ें', '##द', '##ों', 'में', '16', 'र', '##न', 'चाहिए', '।', '19', '##वीं', 'शताब्दी', 'के', 'आर', '##ंभ', 'तक', 'बहुत', 'से', 'लोगों', 'का', 'यह', '##ी', 'मा', '##न', '##ना', 'था', 'कि', 'प्राचीन', 'प', '##त', '##्थ', '##र', ',', 'प', '##ुरा', '##नी', 'बी', '##मा', '##र', '##ियों', 'को', 'दूर', 'करने', 'और', 'स', '##म', '##स', '##्या', '##ओं', 'को', 'स', '##मा', '##प्त', 'करने', 'में', 'स', '##क्ष', '##म', 'होते', 'हैं', '।', 'उनकी', 'इस', 'रचना', 'का', 'वि', '##म', '##ो', '##चन', 'स', '##ो', '##म', '##वार', 'को', 'र', '##ाज', '##भव', '##न', 'में', 'आ', '##चार्य', 'दे', '##व', '##व', '##्र', '##त', 'ने', 'किया', '।', 'केवल', 'छ', '##िं', '##द', '##वा', '##ड़ा', 'सी', '##ट', 'का', '##ंग', '##्रेस', 'के', 'खा', '##ते', 'में', 'आई', 'है', '।', 'ऐसे', 'लोग', 'भ', '##्रा', '##ंत', '##ि', 'फ', '##ै', '##ला', 'कर', 'उसका', 'ला', '##भ', 'उ', '##ठा', '##ना', 'च', '##ाह', '##ते', 'हैं', '।', 'यह', '##ी', 'है', 'द', '##ू', '##षित', 'च', '##ेत', '##ना', 'को', 'पर', '##ि', '##ष', '##्क', '##ृत', 'करके', 'एक', 'ऐसी', 'प', '##वि', '##त्र', 'च', '##ेत', '##ना', 'में', 'पर', '##िव', '##र', '##्ति', '##त', 'कर', 'दे', '##ना', '।', '[SEP]']\n",
      "Precision: 0.09\n",
      "Recall: 0.16\n",
      "F1-Score: 0.11\n"
     ]
    }
   ],
   "source": [
    "indicbert_tokenizer = load_indic_bert_tokenizer()\n",
    "ibert_tokens = indicbert_tokenizer(sentences, max_length=1000, truncation=True)\n",
    "# pred_tokens = tokenizer.convert_ids_to_tokens(ibert_tokens['input_ids'])\n",
    "pred_text = indicbert_tokenizer.decode(ibert_tokens['input_ids'], skip_special_tokens=True)\n",
    "print(pred_tokens)\n",
    "\n",
    "calculate_and_print_metrics(true_tokens, pred_tokens) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd484af",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for WhiteSpace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e2178ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['अलीगढ़', 'मुस्लिम', 'विश्वविद्यालय', 'में', 'कई', 'शिक्षकों', 'ने', 'मौन', 'जुलूस', 'निकाला।एचजीएस', 'धालीवाल', 'आईपीएस', 'दिल्ली', 'में', 'ही', 'पदस्थ', 'हैं।बलदेव', 'और', 'सूबेदार', 'मेजर', 'ब्रह्मू', 'ने', 'कहा', 'कि', 'सीताराम', 'भारद्वाज', 'हमारे', 'पड़ोसी', 'हैं।हर', 'साल', 'मई', 'में', 'भी', 'थोड़ी', 'बहुत', 'बारिश', 'हो', 'जाती', 'थी', 'लेकिन', 'इस', 'बार', 'बादल', 'दूर-दूर', 'ही', 'रहे।', 'इसका', 'असर', 'किसानाें', 'पर', 'साफ', 'दिख', 'रहा', 'है।ऐसे', 'में', 'सरकार', 'की', 'प्राथमिकता', 'रहेगी', 'कि', 'यह', 'जमीन', 'बोर्ड-निगमों', 'को', 'ही', 'बेची', 'जाए', 'ताकि', 'वे', 'यहां', 'अपनी', 'गतिविधियों', 'को', 'बढ़ावा', 'दे', 'सकें।इस', 'सूरत', 'में', 'जो', 'भी', 'नीति', 'प्रमाणित', 'गुणवत्ता', 'वाले', 'शैक्षणिक', 'संस्थानों', 'के', 'अभाव', 'से', 'जूझने', 'का', 'प्रयास', 'न', 'करे', 'वह', 'अप्रासंगिक', 'बनने', 'को', 'अभिशप्त', 'रहेगी।राष्ट्रीय', 'सुरक्षा', 'से', 'जुड़े', 'मुद्दों', 'पर', 'सरकार', 'के', 'साथ', 'एकजुट', 'होने', 'के', 'लिए', 'विपक्ष', 'से', 'अपील', 'करते', 'हुए,', '।कंपनियां', 'अब', 'यूसी', 'सेवा', 'की', 'उपयोगिता', 'समझने', 'लगी', 'हैं', 'इसलिए', 'अब', 'इसमें', 'निवेश', 'भी', 'कर', 'रही', 'हैं।31', 'ओवर', 'के', 'बाद', 'भारत', '243/3,', 'जीत', 'के', 'लिए', '65', 'रन', 'चाहिए', 'आपात्काल', 'मे', 'धेेैर्य', ',', 'अभ्युदय', 'मे', 'क्षमा', ',', 'सदन', 'मे', 'वाक्पटुता', ',', 'युद्ध', 'के', 'समय', 'बहादुरी', ',', 'यशमे', 'अभिरूचि', ',', 'ज्ञान', 'का', 'व्यसन', 'ये', 'सब', 'चीजे', 'महापुरूषोंमे', 'नैसर्गिक', 'रूपसे', 'पायी', 'जाती', 'हैं', '।', 'उपराज्यपाल', 'ने', 'कमीशन', 'ऑफ', 'एनक्वायरी', 'एक्ट', 'का', 'हवाला', 'देते', 'हुए', 'कहा', 'है', 'कि', 'ये', 'कानून', 'सिर्फ', 'केंद्र', 'और', 'राज्य', 'सरकारों', 'को', 'ही', 'जांच', 'आयोग', 'बनाने', 'की', 'इजाजत', 'देता', 'है।', 'हमने', 'सोचा', 'कि', 'कोई', 'चीज', 'नीचे', 'गिर', 'गई', 'है,', 'कोई', 'पटाखा', 'फूटा', 'है', 'या', 'जनरेटर', 'में', 'विस्फोट', 'हो', 'गया', 'है।', 'सिसोदिया', 'के', 'हाथ,', 'आंखों', 'के', 'पास', 'और', 'उनके', 'सरकारी', 'वाहन', 'पर', 'भी', 'स्याही', 'के', 'निशान', 'थे।', 'उत्तराखण्ड', 'आयुर्वेद', 'विश्व', 'विघालय', 'के', 'कालेजों', 'में', 'फीस', 'बढ़ोत्तरी', 'एंव', 'छात्रों', 'के', 'उत्पीड़न', 'के', 'विरोध', 'में', 'उत्तराखण्ड', 'क्रांतिदल', 'के', 'कार्यकताओं', 'ने', 'आज', 'घंटाघर', 'स्थित', 'इंद्रमणी', 'बडोनी', 'की', 'प्रतिमा', 'के', 'समीप', 'धरना', 'प्रदर्शन', 'कर', 'राज्यपाल', 'के', 'नाम', 'ज्ञापन', 'प्रेषित', 'किया।', 'बंगाल', 'सरकार', 'ने', 'जलपाईगुड़ी', 'के', 'रिक्शा', 'चालक', 'की', 'बेटी', 'स्वप्ना', 'के', 'लिए', '10', 'लाख', 'रुपये', 'का', 'पुरस्कार', 'और', 'सरकारी', 'नौकरी', 'की', 'घोषणा', 'की।', 'सिखा', 'के', 'कुल्हे', 'खोल', 'के', 'मैंने', 'उसके', 'पिछवाड़े', 'के', 'छेद', 'पर', 'ये', 'झाग', 'को', 'लगा', 'दिया.', 'गांड', 'के', 'छेद', 'को', 'एकदम', 'चिकना', 'कर', 'के', 'मैंने', 'लंड', 'के', 'ऊपर', 'भी', 'झाग', 'बनाया.', '20/08/2014', 'मुख्य', 'अभियंता,', 'धसान', 'केन', 'कछार', 'सागर', 'के', 'अन्तर्गत', 'गुण', 'नियंत्रण', 'के', 'कार्यों', 'को', 'गति', 'देने', 'हेतु', 'मुख्यालय', 'परिवर्तन।', 'मैंने', 'कभी', 'भी', 'तुम्हारा', 'मित्र', 'के', 'अलावा', 'किसी', 'और', 'तरीके', 'से', 'स्पर्श', 'किया', 'है?', 'अब', 'भगवान', 'को', 'न', 'याद', 'किया', 'तो', 'शायद', 'सिनेमा', 'चौपट', 'हो', 'जाये', 'सो', 'एक', 'भजन', 'भी', 'लाजिमी', 'है।', '22:00', 'IST', '18', 'ओवर', 'के', 'बाद', 'ऑस्ट्रेलिया', 'का', 'स्कोर', '111/5,', 'जीत', 'के', 'लिए', '12', 'गेंदों', 'में', '16', 'रन', 'चाहिए।', '19वीं', 'शताब्दी', 'के', 'आरंभ', 'तक', 'बहुत', 'से', 'लोगों', 'का', 'यही', 'मानना', 'था', 'कि', 'प्राचीन', 'पत्थर,', 'पुरानी', 'बीमारियों', 'को', 'दूर', 'करने', 'और', 'समस्याओं', 'को', 'समाप्त', 'करने', 'में', 'सक्षम', 'होते', 'हैं।', 'उनकी', 'इस', 'रचना', 'का', 'विमोचन', 'सोमवार', 'को', 'राजभवन', 'में', 'आचार्य', 'देवव्रत', 'ने', 'किया।', 'केवल', 'छिंदवाड़ा', 'सीट', 'कांग्रेस', 'के', 'खाते', 'में', 'आई', 'है।', 'ऐसे', 'लोग', 'भ्रांति', 'फैला', 'कर', 'उसका', 'लाभ', 'उठाना', 'चाहते', 'हैं।', 'यही', 'है', 'दूषित', 'चेतना', 'को', 'परिष्कृत', 'करके', 'एक', 'ऐसी', 'पवित्र', 'चेतना', 'में', 'परिवर्तित', 'कर', 'देना।']\n",
      "Precision: 0.29\n",
      "Recall: 0.41\n",
      "F1-Score: 0.34\n"
     ]
    }
   ],
   "source": [
    "pred_tokens = whitespace_tokenize(sentences)\n",
    "print(pred_tokens)\n",
    "\n",
    "calculate_and_print_metrics(true_tokens, pred_tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc330b",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d355411",
   "metadata": {},
   "source": [
    "In this assignment i got the best Precision, Recall and F1-Score with respect to the ground truth(which was determined by me).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
